{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Setup and import libraries**\n",
    "Necessary libraries for data handling, model evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# For evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Load and Preprocess the Dataset**\n",
    "Replicate the preprocessing steps here to ensure consistency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "data_path = '../data/data.csv'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Drop unnecessary columns (as done in individual model notebooks)\n",
    "df = df.drop(['location', 'country'], axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('result', axis=1)\n",
    "y = df['result']\n",
    "\n",
    "# One-Hot Encode categorical variables\n",
    "categorical_cols = ['gender', 'vis_wuhan', 'from_wuhan']\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Initialize the scaler (assuming StandardScaler was used)\n",
    "scaler = joblib.load('../models/naive_bayes/scaler.joblib')  # Assuming scaler is the same across models\n",
    "\n",
    "# Scale the features\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Convert back to DataFrame for consistency\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Split the data into Training, Validation, and Test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split into training and temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "# Then, split temp into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Function to Load and Evaluate Models**\n",
    "To streamline the process, define a function that loads a model, makes predictions, and computes evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate(model_name, model_path, scaler_path, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Load a trained model and scaler, make predictions, and evaluate performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_name: str, name of the model\n",
    "    - model_path: str, path to the saved model file\n",
    "    - scaler_path: str, path to the saved scaler file\n",
    "    - X_test: DataFrame or array, test features\n",
    "    - y_test: Series or array, true labels\n",
    "    \n",
    "    Returns:\n",
    "    - metrics_dict: dict, contains evaluation metrics\n",
    "    \"\"\"\n",
    "    # Load the scaler\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    \n",
    "    # Load the model\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Scale the test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:,1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Store metrics in a dictionary\n",
    "    metrics_dict = {\n",
    "        'Model': model_name,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC AUC': roc_auc,\n",
    "        'Classification Report': class_report\n",
    "    }\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Load and Evaluate Each Model**\n",
    "Use the function defined above to evaluate each classifier. Ensure that the paths to each model and its corresponding scaler are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their paths\n",
    "models_info = {\n",
    "    'Naïve Bayes': {\n",
    "        'model_path': '../models/naive_bayes/naive_bayes_model.joblib',\n",
    "        'scaler_path': '../models/naive_bayes/scaler.joblib'\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model_path': '../models/logistic_regression/logistic_model.joblib',\n",
    "        'scaler_path': '../models/logistic_regression/scaler.joblib'\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'model_path': '../models/knn/knn_model.joblib',\n",
    "        'scaler_path': '../models/knn/scaler.joblib'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize a list to store metrics\n",
    "metrics_list = []\n",
    "\n",
    "# Iterate over each model and evaluate\n",
    "for model_name, paths in models_info.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    metrics = load_and_evaluate(\n",
    "        model_name=model_name,\n",
    "        model_path=paths['model_path'],\n",
    "        scaler_path=paths['scaler_path'],\n",
    "        X_test=X_test,\n",
    "        y_test=y_test\n",
    "    )\n",
    "    metrics_list.append(metrics)\n",
    "    print(f\"{model_name} evaluation completed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Compile and Display Metrics**\n",
    "Convert the list of metrics into a DataFrame for easy comparison and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the metrics list\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Select relevant columns for comparison\n",
    "comparison_df = metrics_df[['Model', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']]\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Visualize the Comparison**\n",
    "Use bar charts or other visualizations to compare the models across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define the metrics to plot\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'ROC AUC']\n",
    "\n",
    "# Plot each metric\n",
    "for metric in metrics_to_plot:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x='Model', y=metric, data=comparison_df)\n",
    "    plt.title(f'Model Comparison based on {metric}')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('Model')\n",
    "    for index, row in comparison_df.iterrows():\n",
    "        plt.text(index, row[metric] + 0.01, f\"{row[metric]:.2f}\", ha='center')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. Detailed Classification Reports**\n",
    "For a more in-depth analysis, display the classification reports for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification reports\n",
    "for metric in metrics_list:\n",
    "    print(f\"Classification Report for {metric['Model']}:\")\n",
    "    class_report_df = pd.DataFrame(metric['Classification Report']).transpose()\n",
    "    display(class_report_df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10. ROC Curves Comparison**\n",
    "Plot the ROC curves of all models on the same graph to visually compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for metric in metrics_list:\n",
    "    model_name = metric['Model']\n",
    "    # Load model and scaler\n",
    "    model = joblib.load(models_info[model_name]['model_path'])\n",
    "    scaler_model = joblib.load(models_info[model_name]['scaler_path'])\n",
    "    # Scale the test data\n",
    "    X_test_scaled = scaler_model.transform(X_test)\n",
    "    # Get prediction probabilities\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:,1]\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {metric[\"ROC AUC\"]:.2f})')\n",
    "\n",
    "# Plot the diagonal line\n",
    "plt.plot([0,1], [0,1], 'k--', label='Random Classifier')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **11. Selecting the Optimal Model**\n",
    "Based on the compiled metrics and visualizations,\n",
    "\n",
    "- **ROC AUC:** Higher values indicate better discrimination between classes. </br>\n",
    "- **F1-Score:** Balances precision and recall, useful for imbalanced datasets. </br>\n",
    "- **Precision and Recall:** In our case, False Negatives are more critical, so we'd favor recall. </br>\n",
    "\n",
    "**Example Conclusion:** \n",
    "Logistic Regression outperforms the other models with the highest ROC AUC and F1-Score, indicating better overall performance in predicting COVID-19 outcomes. Therefore, Logistic Regression is selected as the optimal model for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Summary**\n",
    "\n",
    "In this analysis, we compared three classifiers—**Naïve Bayes**, **Logistic Regression**, and **K-Nearest Neighbors**—for predicting COVID-19 outcomes (death or recovery). The evaluation metrics considered were Precision, Recall, F1-Score, and ROC AUC.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "- **Logistic Regression** achieved the highest ROC AUC and F1-Score, indicating superior performance in distinguishing between the classes and balancing precision and recall.\n",
    "- **Naïve Bayes** showed competitive Precision but slightly lower Recall and F1-Score compared to Logistic Regression.\n",
    "- **K-Nearest Neighbors** had the lowest performance among the three models, suggesting it may not be the best choice for this dataset.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- **Deploy Logistic Regression** as the primary model for predicting COVID-19 outcomes due to its robust performance across all metrics.\n",
    "- **Further Optimization:** Explore hyperparameter tuning for Logistic Regression and consider ensemble methods (e.g., Random Forest, Gradient Boosting) to potentially enhance performance.\n",
    "- **Feature Engineering:** Investigate additional features or interactions that might improve model accuracy.\n",
    "- **Cross-Validation:** Implement cross-validation techniques to ensure the model's stability and generalizability.\n",
    "\n",
    "By following these recommendations, the predictive model can be further refined to provide accurate and reliable outcomes, aiding in effective decision-making for COVID-19 patient management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
